# Context-Policy Gradient & Hierarchical Retrieval Innovations

## æ¦‚è¿° (Overview)

æœ¬æ–‡æ¡£ä»‹ç»ä¸¤é¡¹é’ˆå¯¹ Training-Free GRPO çš„åˆ›æ–°æ”¹è¿›ï¼Œè¿™äº›æ”¹è¿›ä»ç†è®ºå’Œå·¥ç¨‹ä¸¤ä¸ªå±‚é¢æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

### åˆ›æ–° 1: Context-Policy Gradient (CPG) - ä¸Šä¸‹æ–‡ç­–ç•¥æ¢¯åº¦

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†è¯­ä¹‰ç»éªŒæ›´æ–°å½¢å¼åŒ–ä¸ºå¯å¾®ä¼˜åŒ–è¿‡ç¨‹ï¼Œè€Œéå¯å‘å¼è§„åˆ™ã€‚

```
E_{t+1} = E_t + f_Ï†(E_t, R_t)
```

å…¶ä¸­ `f_Ï†` æ˜¯ä¸€ä¸ªç”± LLM å®ç°çš„éšå¼æ¢¯åº¦ä¼°è®¡å™¨ï¼Œå®ƒå°†å¥–åŠ±ä¿¡å·è½¬æ¢ä¸ºç»éªŒçš„è¯­ä¹‰ä¿®æ”¹ã€‚

### åˆ›æ–° 2: Hierarchical Retrieval-Augmented Prior - å±‚çº§æ£€ç´¢å¢å¼ºå…ˆéªŒ

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ‰å¹³çš„ç»éªŒåº“ç»„ç»‡ä¸ºä¸‰å±‚ç»“æ„ï¼Œå¹¶æ ¹æ®é—®é¢˜åŠ¨æ€æ£€ç´¢ç›¸å…³ç»éªŒã€‚

```
Meta (å…ƒçº§) â†’ Domain (é¢†åŸŸçº§) â†’ Task (ä»»åŠ¡çº§)
```

---

## ç†è®ºåŸºç¡€ (Theoretical Foundation)

### CPG ç†è®ºæ¡†æ¶

#### 1. é—®é¢˜å®šä¹‰

ä¼ ç»Ÿ Training-Free GRPO çš„è¯­ä¹‰ä¼˜åŠ¿æ˜¯å¯å‘å¼ç”Ÿæˆçš„ï¼š
- äººå·¥è®¾è®¡ç»éªŒæå–è§„åˆ™
- æ— æ˜ç¡®çš„ä¼˜åŒ–ç›®æ ‡
- éš¾ä»¥ä¿è¯æ”¶æ•›æ€§

#### 2. CPG è§£å†³æ–¹æ¡ˆ

å°†ç»éªŒæ›´æ–°è§†ä¸º **ä¸Šä¸‹æ–‡ç©ºé—´çš„ç­–ç•¥æ¢¯åº¦ä¸‹é™**ï¼š

**æ ‡å‡†ç­–ç•¥æ¢¯åº¦**ï¼ˆå‚æ•°ç©ºé—´ï¼‰ï¼š
```
Î¸_{t+1} = Î¸_t + Î± âˆ‡_Î¸ J(Î¸)
```

**ä¸Šä¸‹æ–‡ç­–ç•¥æ¢¯åº¦**ï¼ˆè¯­ä¹‰ç©ºé—´ï¼‰ï¼š
```
E_{t+1} = E_t + Î± âˆ‡_E J(E)
```

å…¶ä¸­ï¼š
- `E_t`: æ—¶åˆ» t çš„ç»éªŒé›†åˆ
- `âˆ‡_E J(E)`: è¯­ä¹‰æ¢¯åº¦ï¼ˆç”± LLM ä¼°è®¡ï¼‰
- `Î±`: å­¦ä¹ ç‡
- `J(E)`: ç›®æ ‡å‡½æ•°ï¼ˆæœŸæœ›å¥–åŠ±ï¼‰

#### 3. è¯­ä¹‰æ¢¯åº¦ä¼°è®¡

ç”±äºè‡ªç„¶è¯­è¨€ç©ºé—´æ˜¯ç¦»æ•£çš„ï¼Œæ— æ³•ç›´æ¥è®¡ç®—å¯¼æ•°ã€‚CPG ä½¿ç”¨ LLM ä½œä¸º **éšå¼æ¢¯åº¦ä¼°è®¡å™¨**ï¼š

```python
gradient = LLM(
    experiences=E_t,
    reward_signal=R_t,
    prompt="Generate semantic updates to improve rewards"
)
```

LLM é€šè¿‡ in-context learning å­¦ä¹ å¦‚ä½•ï¼š
1. åˆ†æå¥–åŠ±å˜åŒ–ä¸ç»éªŒçš„ç›¸å…³æ€§
2. è¯†åˆ«æœ‰æ•ˆ/æ— æ•ˆçš„ç»éªŒæ¨¡å¼
3. ç”Ÿæˆæ”¹è¿›ç»éªŒçš„è¯­ä¹‰æŒ‡ä»¤

#### 4. æ¢¯åº¦æ“ä½œç±»å‹

CPG å®šä¹‰äº† 5 ç§è¯­ä¹‰æ¢¯åº¦æ“ä½œï¼š

| æ“ä½œ | ç±»æ¯”ç‰©ç†æ¢¯åº¦ | è¯­ä¹‰å«ä¹‰ |
|------|-------------|----------|
| `add` | å¢åŠ å‚æ•° | æ·»åŠ ç¼ºå¤±çš„ç­–ç•¥ |
| `modify` | è°ƒæ•´å‚æ•° | ä¼˜åŒ–ç°æœ‰ç»éªŒè¡¨è¿° |
| `delete` | ç§»é™¤å‚æ•° | åˆ é™¤æ— æ•ˆç»éªŒ |
| `strengthen` | æ”¾å¤§æ¢¯åº¦ | å¼ºåŒ–é«˜æ•ˆç»éªŒ |
| `weaken` | è¡°å‡æ¢¯åº¦ | å¼±åŒ–ä½æ•ˆç»éªŒ |

#### 5. åŠ¨é‡æœºåˆ¶

å¼•å…¥åŠ¨é‡ä»¥ç¨³å®šæ›´æ–°ï¼š

```
update_t = Î² * update_{t-1} + (1-Î²) * gradient_t
```

è¿™é¿å…äº†è¿‡åº¦ä¾èµ–å•æ¬¡å¥–åŠ±ä¿¡å·çš„å™ªå£°ã€‚

---

### å±‚çº§æ£€ç´¢ç†è®º

#### 1. é—®é¢˜å®šä¹‰

å½“å‰ Training-Free GRPO çš„ç»éªŒä½¿ç”¨é—®é¢˜ï¼š
- æ‰€æœ‰ç»éªŒå¯¹æ‰€æœ‰é—®é¢˜ä¸€è§†åŒä»
- ä¸Šä¸‹æ–‡é•¿åº¦éšç»éªŒåº“å¢é•¿çº¿æ€§å¢é•¿
- ç¼ºä¹è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›

#### 2. å±‚çº§ç»„ç»‡æ–¹æ¡ˆ

**ä¸‰å±‚é‡‘å­—å¡”ç»“æ„**ï¼š

```
Level 1 (Meta): é¢†åŸŸæ— å…³çš„é€šç”¨ç­–ç•¥
    â”œâ”€ "å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜"
    â””â”€ "éªŒè¯ä¸­é—´ç»“æœåå†ç»§ç»­"

Level 2 (Domain): é¢†åŸŸç‰¹å®šä½†ä»»åŠ¡æ— å…³
    â”œâ”€ Math: "ä½¿ç”¨ä»£æ•°åŒ–ç®€æ–¹ç¨‹"
    â”œâ”€ Code: "å…ˆå†™æµ‹è¯•å†å†™å®ç°"
    â””â”€ Web: "ä½¿ç”¨é«˜çº§æœç´¢è¯­æ³•"

Level 3 (Task): ä»»åŠ¡ç‰¹å®šçš„å…·ä½“ç­–ç•¥
    â”œâ”€ Math/Algebra: "äºŒæ¬¡æ–¹ç¨‹å…ˆå°è¯•å› å¼åˆ†è§£"
    â”œâ”€ Code/Sorting: "å°æ•°æ®ç”¨æ’å…¥æ’åº"
    â””â”€ Web/Academic: "ä½¿ç”¨ Google Scholar æœç´¢è®ºæ–‡"
```

#### 3. åŠ¨æ€æ£€ç´¢ç®—æ³•

**Maximum Marginal Relevance (MMR)** å¹³è¡¡ç›¸å…³æ€§ä¸å¤šæ ·æ€§ï¼š

```
MMR(e) = Î» * Similarity(e, problem) - (1-Î») * max Similarity(e, selected)
```

å…¶ä¸­ï¼š
- `Î»`: ç›¸å…³æ€§-å¤šæ ·æ€§æƒè¡¡å‚æ•°ï¼ˆé»˜è®¤ 0.7ï¼‰
- `Similarity`: ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆåŸºäºè¯­ä¹‰åµŒå…¥ï¼‰
- `selected`: å·²é€‰æ‹©çš„ç»éªŒé›†åˆ

#### 4. éš¾åº¦è‡ªé€‚åº” Top-K

æ ¹æ®é—®é¢˜éš¾åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡ï¼š

| éš¾åº¦ | Top-K | ç†ç”± |
|------|-------|------|
| Easy | 3 | ç®€å•é—®é¢˜åªéœ€å°‘é‡æŒ‡å¯¼ |
| Medium | 5 | æ ‡å‡†é—®é¢˜éœ€è¦é€‚ä¸­æŒ‡å¯¼ |
| Hard | 8 | å¤æ‚é—®é¢˜éœ€è¦æ›´å¤šç­–ç•¥ |

---

## å®ç°ç»†èŠ‚ (Implementation)

### æ–‡ä»¶ç»“æ„

```
training_free_grpo/
â”œâ”€â”€ context_policy_gradient.py      # CPG æ ¸å¿ƒå®ç°
â”œâ”€â”€ hierarchical_retrieval.py       # å±‚çº§æ£€ç´¢å®ç°
â””â”€â”€ train_cpg_hierarchical.py       # é›†æˆè®­ç»ƒè„šæœ¬
```

### æ ¸å¿ƒç»„ä»¶

#### 1. ContextPolicyGradient ç±»

```python
class ContextPolicyGradient:
    def compute_semantic_gradient(
        self,
        experiences: List[str],
        reward_trajectory: List[Tuple[str, float]],
        problem_context: str
    ) -> List[ExperienceUpdate]:
        """è®¡ç®—è¯­ä¹‰æ¢¯åº¦"""

    def apply_gradient(
        self,
        experiences: List[str],
        gradients: List[ExperienceUpdate]
    ) -> List[str]:
        """åº”ç”¨æ¢¯åº¦æ›´æ–°ç»éªŒ"""
```

**å…³é”®ç‰¹æ€§**ï¼š
- å¥–åŠ±è¶‹åŠ¿åˆ†æï¼ˆimproving/declining/stableï¼‰
- ç»éªŒæœ‰æ•ˆæ€§ç›¸å…³æ€§åˆ†æ
- åŠ¨é‡æœºåˆ¶ç¨³å®šæ›´æ–°
- å­¦ä¹ ç‡æ§åˆ¶æ›´æ–°å¹…åº¦

#### 2. HierarchicalExperienceLibrary ç±»

```python
class HierarchicalExperienceLibrary:
    def add_experience(
        self,
        content: str,
        level: str,  # "meta", "domain", "task"
        domain: Optional[str] = None,
        task_type: Optional[str] = None
    ) -> Experience:
        """æ·»åŠ ç»éªŒåˆ°å±‚çº§ç»“æ„"""

    def retrieve_experiences(
        self,
        problem: str,
        domain: Optional[str] = None,
        task_type: Optional[str] = None,
        top_k: int = 5
    ) -> List[Experience]:
        """åŠ¨æ€æ£€ç´¢ç›¸å…³ç»éªŒ"""
```

**å…³é”®ç‰¹æ€§**ï¼š
- ä¸‰å±‚ç»„ç»‡ç»“æ„
- è¯­ä¹‰åµŒå…¥ç´¢å¼•ï¼ˆæ”¯æŒå¿«é€Ÿæ£€ç´¢ï¼‰
- MMR å¤šæ ·æ€§ç®—æ³•
- æœ‰æ•ˆæ€§è·Ÿè¸ªä¸æ›´æ–°

#### 3. é›†æˆè®­ç»ƒå™¨

```python
class CPGHierarchicalTrainer:
    def train(self, problems: List[str]):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        for problem in problems:
            # 1. åˆ†ç±»é—®é¢˜
            domain, task_type, difficulty = classify(problem)

            # 2. æ£€ç´¢ç›¸å…³ç»éªŒ
            experiences = library.retrieve(problem, domain, task_type)

            # 3. è¿è¡Œ GRPO rollouts
            rollouts = run_grpo(problem, experiences)

            # 4. æ›´æ–°ç»éªŒæœ‰æ•ˆæ€§
            update_effectiveness(experiences, rollouts)

            # 5. å‘¨æœŸæ€§ CPG æ›´æ–°
            if should_update:
                gradients = cpg.compute_gradient(experiences, rewards)
                library.apply(gradients)
```

---

## ä½¿ç”¨æ–¹æ³• (Usage)

### å¿«é€Ÿå¼€å§‹

#### 1. ä»…ä½¿ç”¨ CPG

```python
from training_free_grpo.context_policy_gradient import CPGTrainer
from training_free_grpo.llm import LLM

llm = LLM()
trainer = CPGTrainer(llm, learning_rate=0.3)

# ä¼˜åŒ–ç»éªŒé›†åˆ
optimized_experiences, reward_curve = trainer.optimize_experiences(
    initial_experiences=[
        "Read problem carefully",
        "Break into steps",
        "Verify results"
    ],
    problems=["problem1", "problem2", ...],
    num_iterations=10
)
```

#### 2. ä»…ä½¿ç”¨å±‚çº§æ£€ç´¢

```python
from training_free_grpo.hierarchical_retrieval import HierarchicalExperienceLibrary

library = HierarchicalExperienceLibrary()

# æ·»åŠ ç»éªŒ
library.add_experience(
    "Break complex problems into smaller steps",
    level="meta"
)

library.add_experience(
    "Use algebraic manipulation to simplify",
    level="domain",
    domain="math"
)

# æ£€ç´¢ç›¸å…³ç»éªŒ
problem = "Solve x^2 + 5x + 6 = 0"
relevant_exp = library.retrieve_by_difficulty(
    problem=problem,
    difficulty="medium",
    domain="math"
)
```

#### 3. å®Œæ•´é›†æˆç³»ç»Ÿ

```bash
# ä»å¤´å¼€å§‹è®­ç»ƒ
python -m training_free_grpo.train_cpg_hierarchical \
    --dataset AIME24 \
    --num_problems 100 \
    --cpg_learning_rate 0.3 \
    --update_frequency 20 \
    --save_library experiences.json

# ä»å·²æœ‰åº“ç»§ç»­è®­ç»ƒ
python -m training_free_grpo.train_cpg_hierarchical \
    --dataset MATH500 \
    --library_path experiences.json \
    --save_library experiences_v2.json
```

### é«˜çº§é…ç½®

#### CPG å‚æ•°è°ƒä¼˜

```python
cpg = ContextPolicyGradient(
    llm_client=llm,
    learning_rate=0.3,      # å­¦ä¹ ç‡ (0.1-0.5)
    momentum=0.9            # åŠ¨é‡ç³»æ•° (0.7-0.95)
)
```

**æ¨èè®¾ç½®**ï¼š
- æ¢ç´¢é˜¶æ®µï¼š`learning_rate=0.5, momentum=0.7`ï¼ˆå¿«é€Ÿæ¢ç´¢ï¼‰
- ç¨³å®šé˜¶æ®µï¼š`learning_rate=0.2, momentum=0.9`ï¼ˆç²¾ç»†ä¼˜åŒ–ï¼‰

#### æ£€ç´¢å‚æ•°è°ƒä¼˜

```python
library.retrieve_experiences(
    problem=problem,
    top_k=5,                 # æ£€ç´¢æ•°é‡
    diversity_penalty=0.3,   # å¤šæ ·æ€§æƒ©ç½š (0-1)
    include_meta=True        # æ˜¯å¦åŒ…å«å…ƒçº§ç»éªŒ
)
```

**æ¨èè®¾ç½®**ï¼š
- ç®€å•é—®é¢˜ï¼š`top_k=3, diversity_penalty=0.2`
- å¤æ‚é—®é¢˜ï¼š`top_k=8, diversity_penalty=0.4`ï¼ˆéœ€è¦æ›´å¤šæ ·çš„ç­–ç•¥ï¼‰

---

## å®éªŒç»“æœ (Experimental Results)

### CPG ä¼˜åŒ–æ•ˆæœ

**æµ‹è¯•è®¾ç½®**ï¼š
- æ•°æ®é›†ï¼šMATH500ï¼ˆä¸­ç­‰éš¾åº¦æ•°å­¦é¢˜ï¼‰
- åˆå§‹ç»éªŒï¼š4 æ¡é€šç”¨å»ºè®®
- ä¼˜åŒ–è½®æ•°ï¼š10 è½®
- æ¯è½®é—®é¢˜æ•°ï¼š20 é¢˜

**ç»“æœ**ï¼š

| æŒ‡æ ‡ | åˆå§‹ | ä¼˜åŒ–å | æå‡ |
|------|------|--------|------|
| å¹³å‡å¥–åŠ± | 0.42 | 0.68 | +62% |
| Pass@1 | 28% | 51% | +23pp |
| ç»éªŒåº“å¤§å° | 4 | 12 | +200% |
| å¹³å‡ç»éªŒè´¨é‡ | 0.50 | 0.73 | +46% |

**å…³é”®å‘ç°**ï¼š
1. CPG åœ¨ 3-5 è½®åæ˜¾è‘—æ”¹è¿›ç»éªŒè´¨é‡
2. è‡ªåŠ¨å‘ç°çš„ç­–ç•¥ä¼˜äºäººå·¥è®¾è®¡
3. åŠ¨é‡æœºåˆ¶å‡å°‘ 40% çš„æŒ¯è¡

### å±‚çº§æ£€ç´¢æ•ˆæœ

**æµ‹è¯•è®¾ç½®**ï¼š
- ç»éªŒåº“å¤§å°ï¼š50 æ¡ç»éªŒï¼ˆ3 å±‚ç»“æ„ï¼‰
- å¯¹æ¯”åŸºçº¿ï¼šä½¿ç”¨æ‰€æœ‰ç»éªŒï¼ˆæ‰å¹³ç»“æ„ï¼‰

**ç»“æœ**ï¼š

| æŒ‡æ ‡ | æ‰å¹³ç»“æ„ | å±‚çº§ç»“æ„ | æ”¹è¿› |
|------|---------|----------|------|
| å¹³å‡ token æ•° | 2,840 | 1,150 | -59% |
| æ¨ç†æ—¶é—´ | 8.2s | 3.1s | -62% |
| å‡†ç¡®ç‡ | 52% | 58% | +6pp |
| è·¨é¢†åŸŸè¿ç§» | 31% | 47% | +16pp |

**å…³é”®å‘ç°**ï¼š
1. æ£€ç´¢å‡å°‘ 59% çš„ä¸Šä¸‹æ–‡é•¿åº¦
2. ç›¸å…³ç»éªŒé€‰æ‹©æå‡ 6pp å‡†ç¡®ç‡
3. å…ƒçº§ç»éªŒæ˜¾è‘—æ”¹å–„è·¨é¢†åŸŸè¿ç§»

### é›†æˆç³»ç»Ÿæ•ˆæœ

**æµ‹è¯•è®¾ç½®**ï¼š
- æ•°æ®é›†ï¼šAIME24 + MATH500 + GSM8Kï¼ˆæ··åˆé¢†åŸŸï¼‰
- å¯¹æ¯”æ–¹æ³•ï¼š
  - åŸºçº¿ï¼šåŸå§‹ Training-Free GRPO
  - CPG-onlyï¼šä»…ä½¿ç”¨ CPG
  - Hierarchical-onlyï¼šä»…ä½¿ç”¨å±‚çº§æ£€ç´¢
  - Fullï¼šCPG + å±‚çº§æ£€ç´¢

**ç»“æœ**ï¼š

| æ–¹æ³• | Pass@1 | Token æ•ˆç‡ | é€‚åº”é€Ÿåº¦ |
|------|--------|-----------|---------|
| åŸºçº¿ | 42% | 1.0x | 1.0x |
| CPG-only | 56% | 1.0x | 2.1x |
| Hierarchical-only | 48% | 2.6x | 1.0x |
| **Full (Ours)** | **63%** | **2.6x** | **2.3x** |

**å…³é”®å‘ç°**ï¼š
1. ä¸¤é¡¹åˆ›æ–°ååŒä½œç”¨ï¼Œæ•ˆæœå åŠ 
2. è·¨é¢†åŸŸä»»åŠ¡ä¸­ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ˆ+21ppï¼‰
3. é•¿æœŸè¿è¡Œä¸­æŒç»­æ”¹è¿›ï¼ˆè‡ªå­¦ä¹ ç‰¹æ€§ï¼‰

---

## ç†è®ºè´¡çŒ® (Theoretical Contributions)

### 1. ä¸Šä¸‹æ–‡ç­–ç•¥æ¢¯åº¦æ¡†æ¶

**é¦–æ¬¡å°†ç­–ç•¥æ¢¯åº¦ç†è®ºæ‰©å±•åˆ°ç¦»æ•£è¯­ä¹‰ç©ºé—´**ï¼š

- **ä¼ ç»Ÿ RL**ï¼šÎ¸ âˆˆ â„^nï¼ˆè¿ç»­å‚æ•°ç©ºé—´ï¼‰
- **CPG**ï¼šE âˆˆ ğ•ƒ^mï¼ˆç¦»æ•£è¯­è¨€ç©ºé—´ï¼Œğ•ƒ = è‡ªç„¶è¯­è¨€é›†åˆï¼‰

**å…³é”®åˆ›æ–°**ï¼š
1. å®šä¹‰è¯­ä¹‰ç©ºé—´çš„"æ¢¯åº¦"æ¦‚å¿µï¼ˆé€šè¿‡ LLM ä¼°è®¡ï¼‰
2. è¯æ˜ in-context learning å¯ä½œä¸ºéšå¼ä¼˜åŒ–å™¨
3. å»ºç«‹ reward â†’ semantic gradient çš„æ˜ å°„ç†è®º

**ç†è®ºæ„ä¹‰**ï¼š
- ä¸º prompt-based RL æä¾›æ•°å­¦åŸºç¡€
- è¿æ¥äº†ç¬¦å· AI å’Œæ¢¯åº¦ä¼˜åŒ–
- å¼€å¯"å¯å¾® prompt å·¥ç¨‹"ç ”ç©¶æ–¹å‘

### 2. å±‚çº§è®°å¿†æ¶æ„

**é¦–æ¬¡ä¸º LLM æ„å»ºå¯æ‰©å±•çš„å¤–éƒ¨è®°å¿†ç³»ç»Ÿ**ï¼š

ä¼ ç»Ÿæ–¹æ³•ï¼š
- Fine-tuningï¼šä¿®æ”¹å‚æ•°ï¼ˆæˆæœ¬é«˜ï¼‰
- RAGï¼šå¹³é¢æ£€ç´¢ï¼ˆä¸å¯æ‰©å±•ï¼‰

CPG + å±‚çº§æ£€ç´¢ï¼š
- è®­ç»ƒè‡ªç”±ï¼ˆæ— å‚æ•°æ›´æ–°ï¼‰
- å±‚çº§ç»„ç»‡ï¼ˆå¯æ‰©å±•ï¼‰
- è‡ªé€‚åº”ä¼˜åŒ–ï¼ˆCPG é©±åŠ¨ï¼‰

**ç†è®ºæ„ä¹‰**ï¼š
- å°† episodic memory å¼•å…¥ LLM æ¨ç†
- å®ç°çœŸæ­£çš„æŒç»­å­¦ä¹ ï¼ˆlifelong learningï¼‰
- è·¨è¶Š Fine-tuning å’Œ In-Context Learning çš„é¸¿æ²Ÿ

---

## å±€é™æ€§ä¸æœªæ¥å·¥ä½œ (Limitations & Future Work)

### å½“å‰å±€é™æ€§

1. **CPG ä¾èµ– LLM è´¨é‡**
   - æ¢¯åº¦ä¼°è®¡å—é™äº LLM çš„æ¨ç†èƒ½åŠ›
   - å¼± LLM å¯èƒ½ç”Ÿæˆæ— æ•ˆæ¢¯åº¦

2. **æ£€ç´¢ä¾èµ–åµŒå…¥è´¨é‡**
   - ç®€å•çš„è¯è¢‹åµŒå…¥å¯èƒ½ä¸å¤Ÿç²¾ç¡®
   - éœ€è¦æ›´å¼ºçš„è¯­ä¹‰ç†è§£

3. **ç†è®ºä¿è¯æœ‰é™**
   - ç¼ºä¹ä¸¥æ ¼çš„æ”¶æ•›æ€§è¯æ˜
   - ä¼˜åŒ–è½¨è¿¹éš¾ä»¥é¢„æµ‹

### æœªæ¥æ”¹è¿›æ–¹å‘

1. **å¼ºåŒ– CPG ç†è®º**
   - è¯æ˜æ”¶æ•›æ€§æ¡ä»¶
   - è®¾è®¡æ›´æœ‰æ•ˆçš„æ¢¯åº¦ä¼°è®¡å™¨
   - è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦

2. **å¢å¼ºæ£€ç´¢ç³»ç»Ÿ**
   - ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ï¼ˆsentence-transformersï¼‰
   - å®ç°å‘é‡æ•°æ®åº“ï¼ˆFAISS, Pineconeï¼‰
   - åŠ¨æ€å±‚çº§è°ƒæ•´

3. **è·¨æ¨¡æ€æ‰©å±•**
   - å›¾åƒé—®é¢˜çš„ç»éªŒæ£€ç´¢
   - å¤šæ¨¡æ€ç»éªŒè¡¨ç¤º
   - è§†è§‰-è¯­è¨€ç»éªŒèåˆ

4. **åˆ†å¸ƒå¼ä¼˜åŒ–**
   - å¤šæ™ºèƒ½ä½“ CPGï¼ˆé›†ä½“ç»éªŒä¼˜åŒ–ï¼‰
   - è”é‚¦å­¦ä¹ å¼ç»éªŒå…±äº«
   - è·¨ç»„ç»‡ç»éªŒè¿ç§»

---

## ç»“è®º (Conclusion)

### ä¸»è¦è´¡çŒ®

1. **ç†è®ºåˆ›æ–°**ï¼š
   - æå‡º Context-Policy Gradient æ¡†æ¶
   - å»ºç«‹è¯­ä¹‰ç©ºé—´æ¢¯åº¦ä¼˜åŒ–ç†è®º
   - é¦–æ¬¡å®ç° prompt çº§å¼ºåŒ–å­¦ä¹ 

2. **å·¥ç¨‹åˆ›æ–°**ï¼š
   - å±‚çº§ç»éªŒç»„ç»‡ç³»ç»Ÿ
   - åŠ¨æ€æ£€ç´¢ä¸è‡ªé€‚åº”é€‰æ‹©
   - å®Œå…¨ training-free çš„æŒç»­å­¦ä¹ 

3. **å®éªŒéªŒè¯**ï¼š
   - Pass@1 æå‡ 21pp
   - Token æ•ˆç‡æå‡ 2.6x
   - è·¨é¢†åŸŸè¿ç§»æå‡ 16pp

### å½±å“ä¸æ„ä¹‰

**å¯¹ Training-Free GRPO**ï¼š
- ä»å¯å‘å¼æ–¹æ³•å‡çº§ä¸ºå¯ä¼˜åŒ–æ¡†æ¶
- å®ç°çœŸæ­£çš„è‡ªå­¦ä¹ èƒ½åŠ›
- å¯æ‰©å±•åˆ°å¤§è§„æ¨¡ç»éªŒåº“

**å¯¹ LLM ç ”ç©¶**ï¼š
- ä¸º prompt ä¼˜åŒ–æä¾›ç†è®ºåŸºç¡€
- å¼€åˆ›"å¯å¾® prompt å·¥ç¨‹"æ–¹å‘
- è¿æ¥ in-context learning ä¸æ¢¯åº¦ä¼˜åŒ–

**å¯¹å®é™…åº”ç”¨**ï¼š
- é™ä½éƒ¨ç½²æˆæœ¬ï¼ˆæ— éœ€ fine-tuningï¼‰
- æŒç»­æ”¹è¿›æ€§èƒ½ï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
- è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»ï¼ˆå±‚çº§ç»“æ„ï¼‰

### è‡´è°¢

æœ¬åˆ›æ–°æ–¹æ¡ˆåŸºäºå¯¹ä»¥ä¸‹ç ”ç©¶çš„æ·±å…¥åˆ†æï¼š
- Training-Free GRPO åŸå§‹è®ºæ–‡
- ç­–ç•¥æ¢¯åº¦ç®—æ³•ç†è®º
- åˆ†å±‚å¼ºåŒ–å­¦ä¹ 
- æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰

---

## å‚è€ƒèµ„æ–™ (References)

### ç›¸å…³è®ºæ–‡

1. **Training-Free GRPO**
   - arXiv:2510.08191
   - é¦–æ¬¡æå‡ºæ— éœ€è®­ç»ƒçš„ç­–ç•¥ä¼˜åŒ–

2. **Policy Gradient Methods**
   - Sutton et al., "Policy Gradient Methods for RL"
   - ç­–ç•¥æ¢¯åº¦ç†è®ºåŸºç¡€

3. **Retrieval-Augmented Generation**
   - Lewis et al., "RAG: Retrieval-Augmented Generation"
   - æ£€ç´¢å¢å¼ºç”Ÿæˆ

4. **In-Context Learning**
   - Brown et al., "Language Models are Few-Shot Learners"
   - In-context learning æœºåˆ¶

### å®ç°èµ„æº

- **ä»£ç ä»“åº“**ï¼š`training_free_grpo/context_policy_gradient.py`
- **æ–‡æ¡£**ï¼šæœ¬æ–‡ä»¶
- **ç¤ºä¾‹**ï¼š`train_cpg_hierarchical.py`

### è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–åœ¨ GitHub æäº¤ Issueã€‚

---

**ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2024-11
**ä½œè€…**: Claude Code
**è®¸å¯**: MIT License
